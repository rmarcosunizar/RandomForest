---
title: "Random Forest"
output: html_notebook
---

###Introducción a Random Forest

**Random Forest** es un algoritmo de predicción que utiliza un conjunto de normas binarias para calcular un valor objetivo. Este algoritmo puede ser utilizado para clasificación (variables categórica) o regresión (variables continuas). 

El algoritmo **Random Forest** esta basado en el método de árboles de decisión (CART - *Classification and Regression Trees*).


![](/home/marcos/Imágenes/random_forest.png "Ejemplo Random Forest")

El algoritmo generador de árboles:

  * Determina que variable utilizar para dividir un nodo y el valor a utilizar.
  * Decide cuando parar (nodo terminal) o continuar la división.
  * Utiliza diferentes aproximaciones para determinar el mejor umbral de separación en un nodo.
  * Asigna una clase a cada terminación.


**Random Forest** es un clasificador de ensamblado (*ensemble classifier*). Este tipo de clasificadores se basan en calibrar varios submodelos, en este caso árboles de decisión, y posteriormente agregar los resultados de los distintos modelos para determinar la `predicción final`.

Además, RF provee información sobre la precisión del modelo y la importancia de las variables predictivas.


###Consideraciones específicas de funcionamiento

1. Para cada uno de los árboles ajustados en el modelo, e algoritmo selecciona `aleatoriamente` 2/3 del conjunto total de datos (`población`) como muestra de calibración en sucesivos `pasos` (cada paso representa un árbol de decisión) para ajustar el modelo.

2. La muestra restante (denominada *Out-Of-Bag* o `OOB`). Esta muestra es usada por el propio algoritmo para medir el `error` y la `importancia de las variables`.

3. La asignación del valor final (**predicción**) se hace en función de los `votos` obtenidos de los diferentes árboles.

Cada árbol es construido usando el siguiente proceso:

  * Sea *N* el numero de casos de prueba, *M* es el numero de `variables` en el modelo.
  * Sea *m* el numero de variables de entrada a ser usado para determinar la decisión en un `nodo` dado ($m<M$).
  * Elegir un conjunto de entrenamiento para este árbol y usar el resto de los casos (`OOB`) de prueba para estimar el error.
  * Para cada `nodo` del árbol, elegir aleatoriamente *m* variables en las cuales basar la decisión. Calcular la mejor partición a partir de las *m* variables del conjunto de entrenamiento.
  
Para la predicción un nuevo caso es empujado hacia abajo por el árbol. Luego se le asigna la etiqueta del nodo terminal donde termina. Este proceso es iterado por todos los arboles en el ensamblado, y la etiqueta que obtenga la mayor cantidad de incidencias es reportada como la predicción.


###Random Forest en R

El paquete `randomForest` provee las funciones para trabajar con modelos Ranfom Forest en R.

Principalmente nos centraremos en usar la función `randomForest()`, que es la que permite al ajuste de modelos de regresión o clasificación.

```{r}
#install.packages('randomForest')
library(randomForest)
help(randomForest)
```

La sintaxis del argumento `formula` es la misma que la vista en otros modelos de regresión.

No obstante existen otros argumentos específicos como:

  * `mtry`: número de variables consideradas para separar cada nodo.
  * `importance`: determina si se calculo o no la importancia de las variables predictivas en el modelo.
  * `ntree`: número de árboles utilizados para generar la predicción. 
  
  
###Rescatando el script de regresión espacial

Antes de empezar con **Random Foreest** vamos a recuperar el trabajo previo que hicimos de análisis de regresión. Este será nuestro punto de partida para empezar con RF. Básicamente se trata de reemplazar el método de `regresión lineal` por `regresión con RF`.


####Paso 1. Carga de librerías

```{r}
#Librerias para poder trabajar con capas vectoriales y raster
# install.packages('maptools')
library(maptools)
# install.packages('raster')
library(raster)
#install.packages('rgdal')
library(rgdal)

```


####Paso 2. Lectura de capas raster - Variables independientes

Ten en cuenta que en este ejemplo las capas raster se encuentran en una carpeta llamada `variables` dentro del directorio de trabajo.

```{r}
#Establecemos el directorio de trabajo
path <- '/media/marcos/Elements/Asignaturas Master TIG/3.6 Modelos parametricos y no parametricos/Regresion_R/Practica_1/variables'

lista_variables <- list.files(path=path,pattern='*.txt', full.names=TRUE)
lista_variables

```

```{r}
variables <- stack(lista_variables)
head(variables)

```

####Paso 3. Lectura de capa Shapefile de estaciones

```{r}
library(rgdal)
datos<-readOGR("/media/marcos/Elements/Asignaturas Master TIG/3.6 Modelos parametricos y no parametricos/Regresion_R/Practica_1/","tdatos_jun")
plot(datos)

```

####Paso 4. Extraer coordenadas, variable dependiente e independientes

```{r}
#Leer las coordenados de los puntos
coord<-datos@coords

#Extraer las variables independientes usando las coordendas y las capas raster
datos_regr<-extract(variables, coord)

#Extraer la variable dependiente y crear el data.frame para la regresión
training<-data.frame(cbind(datos_regr,datos$rfo_MES))
names(training)<-c(names(variables),"rfo_MES")

```

####Paso 5. Ajuste del modelo de regresión lineal

```{r}

#Ajuste del modelo usando todas las variables independientes
lm.rfo<-lm(rfo_MES ~ ., data=training)
summary(lm.rfo)

#Predicción para crear la capa raster de temperaturas medias
lm.rfo.pred<-predict(variables, lm.rfo, type="response", index=1, progress="window")
plot(lm.rfo.pred)

```

###Ahora con Random Forest

Básicamente podemos reciclar **TODO** el código cambiando únicamente las 2 últimas instrucciones. Es decir, la función para ajustar el modelo, sustituyendo la función de `regresión lineal` por la de `Random Forest`; y el resultado de la predicción, simplemente para no sobreescribir la predicción hecha anteriormente:

```{r}
rfo.rfo<-randomForest(rfo_MES ~ ., data=training, mtry=3, importance=TRUE, ntree=500)

rfo.rfo.pred<-predict(variables, rfo.rfo, type="response", index=1, progress="window")
plot(lm.rfo.pred)

```

###¿Qué modelo funciona mejor?

Vamos a comparar mediante diagramas de dispersión los 2 modelos:

```{r}

#Obtenemos la predicción en los puntos de muestreo
val.lm<-predict(lm.tmed, training)
val.lm<-cbind(training$Tmed_MES,val.lm)
  
#Diagrama de dispersión de la regresión lineal
plot(val.lm[,1],val.lm[,2],main="Regresión lineal")
abline(lm(val.lm[,1] ~ val.lm[,2]))
summary(lm(val.lm[,1] ~ val.lm[,2]))

```


```{r}

#Obtenemos la predicción en los puntos de muestreo
val.rfo<-predict(rfo.tmed, training)
val.rfo<-cbind(training$Tmed_MES,val.rfo)
  
#Diagrama de dispersión de la regresión lineal
plot(val.rfo[,1],val.rfo[,2],main="Regresión lineal")
abline(lm(val.rfo[,1] ~ val.rfo[,2]))
summary(lm(val.rfo[,1] ~ val.rfo[,2]))

```


####¿Y para clasificación?

Muy sencillo, basta con que la variable dependiente sea una categoria (`factor`) en lugar de un valor numérico.

```{r}
#CARGA LA LIBRERIAS
library(randomForest) #RANDOM FOREST
require(raster)

#LISTADO DE VARIABLES
lista_variables <- list.files(path="/media/marcos/Elements/Asignaturas Master TIG/3.6 Modelos parametricos y no parametricos/Regresion_R/practica_final/bandas",pattern='*.txt', full.names=TRUE)

lista_variables

variables <- stack(lista_variables)

#IMPORTA LA TABLA DE PRESENCIAS
datos<-read.table("/media/marcos/Elements/Asignaturas Master TIG/3.6 Modelos parametricos y no parametricos/Regresion_R/practica_final/ifn3_clc_fcc70.csv",header=T, sep=';')
coord<-datos[,2:3]
names(coord)<-c("x","y")
tabla_datos<-extract(variables, coord)
tabla_clas<-data.frame(cbind(tabla_datos,datos))
tabla_clas<-subset(tabla_clas, select = -c(POINT_X,POINT_Y))
tabla_clas[tabla_clas==0] = NA
tabla_clas<-na.omit(tabla_clas)

```

```{r}
#AJUSTE DEL MODELO RANDOM FORESTS
clas_rfo<-randomForest(Nombre ~ ., data=tabla_clas, mtry=3, importance=TRUE, ntree=1800)
clas_rfo

```

```{r}
#PREDICCI?N GEOGR?FICA
clas_rfo_pred<-predict(variables, clas_rfo,type="response", index=1, progress="window")
plot(clas_rfo_pred)

```


####Evaluación

Implementa un script de clasificación de imagen usando `Random Forest'. Utiliza el código del ejemplo anterior para jugar con los parámetros y optimizar el resultado (minimizar el error OOB).
